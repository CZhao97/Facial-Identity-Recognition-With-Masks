{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Face_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time face detection & ID identification model for fast check-in."
      ],
      "metadata": {
        "id": "LyPFo1Fgm8IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages"
      ],
      "metadata": {
        "id": "IHS87y3cnSFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyYCOhA9ei9M",
        "outputId": "106b96b9-a9fd-47b1-b4cd-1b76ad687566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision->facenet-pytorch) (3.10.0.2)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n",
            "Collecting mmcv\n",
            "  Downloading mmcv-1.4.8.tar.gz (498 kB)\n",
            "\u001b[K     |████████████████████████████████| 498 kB 15.4 MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv) (21.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv) (3.13)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv) (3.0.7)\n",
            "Building wheels for collected packages: mmcv\n",
            "  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv: filename=mmcv-1.4.8-py2.py3-none-any.whl size=762284 sha256=835b79792a2dca7af06e869c67f79f4a19f01e0dc6319ccf3213c514bc1877be\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/a5/24/3bc956c9c1330a563691cfc6905df8e6b142ea0740b6f6fa87\n",
            "Successfully built mmcv\n",
            "Installing collected packages: yapf, addict, mmcv\n",
            "Successfully installed addict-2.4.0 mmcv-1.4.8 yapf-0.32.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "pip install facenet-pytorch\n",
        "pip install mmcv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "S2mPaeV6naPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, Image\n",
        "from IPython import display as dis\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from IPython.core.display import Video\n",
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "import mmcv, cv2\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from skimage.morphology import disk\n",
        "from skimage.filters.rank import autolevel\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SBvpY_rdeoLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edb95cd-cea7-4900-d488-915a2a986fe3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FastMTCNN(object):\n",
        "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
        "    \n",
        "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
        "        \"\"\"Constructor for FastMTCNN class.\n",
        "        \n",
        "        Arguments:\n",
        "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
        "                and remembered for `stride-1` frames.\n",
        "        \n",
        "        Keyword arguments:\n",
        "            resize (float): Fractional frame scaling. [default: {1}]\n",
        "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
        "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
        "        \"\"\"\n",
        "        self.stride = stride\n",
        "        self.resize = resize\n",
        "        self.mtcnn = MTCNN(*args, **kwargs)\n",
        "        \n",
        "    def __call__(self, frames):\n",
        "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
        "        if self.resize != 1:\n",
        "            frames = [\n",
        "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
        "                    for f in frames\n",
        "            ]\n",
        "                      \n",
        "        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
        "\n",
        "        faces = []\n",
        "        for i, frame in enumerate(frames):\n",
        "            box_ind = int(i / self.stride)\n",
        "            if boxes[box_ind] is None:\n",
        "                continue\n",
        "            for box in boxes[box_ind]:\n",
        "                box = [int(b) for b in box]\n",
        "                faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
        "        \n",
        "        return faces"
      ],
      "metadata": {
        "id": "jyiKcDvYAV_l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))\n",
        "# currently using, for face detection\n",
        "mtcnn = MTCNN(keep_all=True, min_face_size=224, device=device)\n",
        "# A faster model, for future performance improvment \n",
        "mtcnn_fast = FastMTCNN(keep_all=True, min_face_size=224, device=device,stride=4)"
      ],
      "metadata": {
        "id": "S5WwCg6O7Uwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bda306-d380-4447-8ca2-b806415d96f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Extracting training data from input videos."
      ],
      "metadata": {
        "id": "H6LqwC-MdH67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_video = files.upload() # For uploading training video to colab, click cancel if have no file to upload\n",
        "user_name = \"ROY\"          #User name of the above uploaded video"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "id": "YGSswGlMdHFN",
        "outputId": "fe7411ba-480b-4847-cfd4-3f0102b9633d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ed04b5bb-f128-4e67-9824-e68cd7a7e7c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ed04b5bb-f128-4e67-9824-e68cd7a7e7c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frames extraction: Extract each frame of user face, save to google drive"
      ],
      "metadata": {
        "id": "i-2_5mK_EPI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video = mmcv.VideoReader('/content/roy_glasses.mov') #Select training video here\n",
        "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video] #Extracting frames from video\n",
        "frames_tracked = []\n",
        "path = '/content/drive/MyDrive/ut/MIE1517_group_project/ImageDataset/ROY'               #Put in the output directory you want \n",
        "os.mkdir(path)\n",
        "os.chdir(path)\n",
        "for i, frame in enumerate(frames):\n",
        "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
        "    img = np.array(frame)    \n",
        "    faces, _ = mtcnn.detect(frame)                                           #Return coordinates of faces from mtcnn model\n",
        "\n",
        "    if i%5==0:\n",
        "\n",
        "      if faces is not None: \n",
        "        for (x,y,w,h) in faces:\n",
        "          x_w_diff = int(w-x)\n",
        "          y_h_diff = int(h-y)\n",
        "          if x_w_diff > 223. and y_h_diff >223.:\n",
        "            x_w_mid = int(x+(w-x)/2)\n",
        "            y_h_mid = int(y+(h-y)/2)\n",
        "            selected_x = x_w_mid - 155\n",
        "            selected_w = x_w_mid + 155\n",
        "            selected_y = y_h_mid - 260\n",
        "            selected_h = y_h_mid + 50           \n",
        "            selected_img = img[selected_y:selected_h, selected_x:selected_w, :] #Adjustment for cropping the correct face images.\n",
        "\n",
        "            faces = Image.fromarray(selected_img).resize((224,224))             #Resizing the image to desired input size.\n",
        "      \n",
        "            gray_img = cv2.cvtColor(np.array(faces), cv2.COLOR_RGB2GRAY)        #Converting to Gray Scale images for training\n",
        "            cv2.imwrite(user_name + str(i) + '.png', gray_img)\n",
        "            #plt.imshow(gray_img, cmap = 'gray')                                #Displaying images only for developing, commented out in actual practice.\n",
        "            #plt.show() \n",
        "    else:\n",
        "      continue                                                     \n",
        "          \n",
        "\n",
        "print('\\nDone')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFUoWZkXfN80",
        "outputId": "387cf36f-90f1-4506-8d3e-fa8c2248ef5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking frame: 86\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ID identification MODEL and TRAINING method CAN be placed below."
      ],
      "metadata": {
        "id": "xF_d-_BTJiS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(dataset_path, batch_size=64):\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((224, 224)),  # (224, 224)\n",
        "                                    # transforms.ColorJitter(), # change image color\n",
        "                                    transforms.RandomHorizontalFlip(),  # flip images\n",
        "                                    # transforms.RandomAffine(30), # Random affine transformation of the image keeping center invariant.\n",
        "                                    # transforms.CenterCrop(224),\n",
        "                                    # TODO: standardization\n",
        "                                    transforms.ToTensor()])\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize([224, 224]), transforms.CenterCrop(224), transforms.ToTensor()])\n",
        "\n",
        "    training_dataset = datasets.ImageFolder(root = dataset_path, transform=transform)\n",
        "\n",
        "    indices = list(range(len(training_dataset)))\n",
        "\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_train_val = int(0.8 * len(indices))\n",
        "\n",
        "    train_indices, val_indices = indices[:split_train_val], indices[split_train_val:]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader, val_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, sampler=train_sampler), \\\n",
        "                  torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "                    \n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "yQzI9XK5JsIw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model to adapt Alex Model\n",
        "# class AlexNetModel(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(AlexNetModel, self).__init__()\n",
        "#         self.name = 'AlexNetModel'\n",
        "#         self.conv1 = nn.Conv2d(256, 30, 3)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.fc1 = nn.Linear(120, 64)\n",
        "#         self.fc2 = nn.Linear(64, 8)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = x.view(-1, 120)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "class AlexNetModel(nn.Module):\n",
        "\n",
        "    def __init__(self, outputs):\n",
        "        super(AlexNetModel, self).__init__()\n",
        "        self.name = 'AlexNetModel'\n",
        "        self.conv1 = nn.Conv2d(256, 30, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(120, 64)\n",
        "        self.fc2 = nn.Linear(64, outputs*2)\n",
        "        self.softmax = m = nn.Softmax(dim=1)\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 120)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        mean, variance = torch.split(x, self.outputs, dim=1)\n",
        "        variance = F.softplus(variance) + 1e-6\n",
        "        return mean, variance\n"
      ],
      "metadata": {
        "id": "F7E6MnQJVY7E"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianMixtureAlex(nn.Module):\n",
        "    \"\"\" Gaussian mixture MLP which outputs are mean and variance.\n",
        "\n",
        "    Attributes:\n",
        "        models (int): number of models\n",
        "        inputs (int): number of inputs\n",
        "        outputs (int): number of outputs\n",
        "        hidden_layers (list of ints): hidden layer sizes\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_models=5, outputs=1):\n",
        "        super(GaussianMixtureAlex, self).__init__()\n",
        "        self.name = 'GaussianMixtureAlex'\n",
        "        self.num_models = num_models\n",
        "        self.outputs = outputs\n",
        "        for i in range(self.num_models):\n",
        "            model = AlexNetModel(outputs=self.outputs)\n",
        "            setattr(self, 'model_'+str(i), model)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # connect layers\n",
        "        means = []\n",
        "        variances = []\n",
        "        for i in range(self.num_models):\n",
        "            model = getattr(self, 'model_' + str(i))\n",
        "            mean, var = model(x)\n",
        "            means.append(mean)\n",
        "            variances.append(var)\n",
        "        means = torch.stack(means)\n",
        "        mean = means.mean(dim=0)\n",
        "        variances = torch.stack(variances)\n",
        "        variance = (variances + means.pow(2)).mean(dim=0) - mean.pow(2)\n",
        "        return mean, variance "
      ],
      "metadata": {
        "id": "BbXE2d7CUJ9E"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to save checkpoint\n",
        "\n",
        "def OneHotEncoder(ls):\n",
        "  unique_ls = list(set(ls))\n",
        "  onehot_encoded = np.zeros((len(ls), 8))\n",
        "  for i in range(len(ls)):\n",
        "    onehot_encoded[i][ls[i]] = 1\n",
        "  return onehot_encoded\n",
        "\n",
        "def save_checkpoint(model, batch_size, lr, epoch):\n",
        "  model_path = \"{}_{}_{}_{}\".format(model.name, batch_size, lr, epoch)\n",
        "  torch.save(model, model_path)\n",
        "  print('Checkpoint of {} has been stored successfully!',format(model_path))\n",
        "\n",
        "def NLLloss(y, mean, var):\n",
        "    \"\"\" Negative log-likelihood loss function. \"\"\"\n",
        "    return (torch.log(var) + ((y - mean).pow(2))/var).sum() \n",
        "\n",
        "def get_accuracy(model, data_loader, grey_images_flag = False):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "  for imgs, labels in data_loader:\n",
        "    #############################################\n",
        "    #To Enable GPU Usage\n",
        "\n",
        "    if grey_images_flag:\n",
        "\n",
        "      grey_images = torchvision.transforms.Grayscale()(imgs)\n",
        "\n",
        "      imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
        "\n",
        "    imgs = alexnet.features(imgs)\n",
        "\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    #############################################\n",
        "    \n",
        "    mean, _ = model(imgs)\n",
        "    \n",
        "    #select index with maximum prediction score\n",
        "    pred = mean.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "    total += imgs.shape[0]\n",
        "  return correct / total\n",
        "\n",
        "def predict(model, data_loader, grey_images_flag = False):\n",
        "\n",
        "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "  for imgs, labels in data_loader:\n",
        "    #############################################\n",
        "    #To Enable GPU Usage\n",
        "\n",
        "    if grey_images_flag:\n",
        "\n",
        "      grey_images = torchvision.transforms.Grayscale()(imgs)\n",
        "\n",
        "      imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
        "\n",
        "    imgs = alexnet.features(imgs)\n",
        "\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    #############################################\n",
        "    \n",
        "    mean, var = model(imgs)\n",
        "    \n",
        "    #select index with maximum prediction score\n",
        "    pred = torch.argmax(mean, dim=1)\n",
        "\n",
        "    pred = pred.cpu().detach().numpy()\n",
        "\n",
        "    var = var.cpu().detach().numpy()\n",
        "\n",
        "    variance = [var[i][val] for i, val in enumerate(pred)]\n",
        "\n",
        "    std = np.sqrt(variance)\n",
        "  return pred, variance, std\n",
        "\n",
        "\n",
        "def train(model, dataset_path, opt, batch_size = 64, learning_rate=0.01, epochs=30, grey_images_flag = False):\n",
        "\n",
        "  #############################################\n",
        "  # input: grey_images_flag: boolean if gray image conversion is needed\n",
        "  #############################################\n",
        "\n",
        "  train_loader, val_loader = split_data(dataset_path, batch_size = batch_size)\n",
        "\n",
        "  criterion = NLLloss\n",
        "\n",
        "  # criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = opt(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "  iters, losses, train_acc, val_acc = [], [], [], []\n",
        "  \n",
        "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "  n = 0 # the number of iterations\n",
        "  for epoch in range(epochs):\n",
        "    for imgs, labels in tqdm(iter(train_loader)):\n",
        "\n",
        "      if grey_images_flag:\n",
        "\n",
        "        grey_images = torchvision.transforms.Grayscale()(imgs)\n",
        "\n",
        "        imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
        "\n",
        "      imgs = alexnet.features(imgs)\n",
        "\n",
        "      #############################################\n",
        "      #To Enable GPU Usage\n",
        "      if use_cuda and torch.cuda.is_available():\n",
        "        imgs = imgs.cuda()\n",
        "\n",
        "        one_hot_labels = torch.from_numpy(OneHotEncoder(labels))\n",
        "\n",
        "        labels = one_hot_labels.cuda()\n",
        "      #############################################\n",
        "  \n",
        "      mean, var = model(imgs) \n",
        "      # print(mean.shape)\n",
        "      # print(var.shape)\n",
        "      # print(labels.shape)\n",
        "\n",
        "      # output = model(imgs)\n",
        "\n",
        "      loss = NLLloss(labels, mean, var)\n",
        "\n",
        "      # loss = criterion(output, labels)\n",
        "\n",
        "      # loss = criterion(m(mean), labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    # save the current training information\n",
        "    iters.append(n)\n",
        "    losses.append(float(loss)/batch_size)\n",
        "    train_acc.append(get_accuracy(model, train_loader, grey_images_flag = grey_images_flag))\n",
        "    val_acc.append(get_accuracy(model, val_loader, grey_images_flag = grey_images_flag))\n",
        "    n += 1\n",
        "      \n",
        "    if ((epoch+1) % 25 == 0):\n",
        "      save_checkpoint(model, batch_size, learning_rate, epoch+1)\n",
        "\n",
        "  # plotting\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, train_acc, label=\"Train\")\n",
        "  plt.plot(iters, val_acc, label=\"Validation\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Training Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "  print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "metadata": {
        "id": "6HqngQ4wVjgq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianMixtureAlex(outputs = 8)\n",
        "\n",
        "data_path = '/content/drive/MyDrive/ut/MIE1517_group_project/ImageDataset/'\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "  \n",
        "train(model, data_path, opt = optim.Adam, batch_size = 32, learning_rate=0.001, epochs=50, grey_images_flag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRfl6tg9WMDe",
        "outputId": "81431ee8-1023-4f05-e01f-863745f7996b"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.02s/it]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.02it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.07s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.00it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.10s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.01s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.01s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.01it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.08s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.01s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.07s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint of {} has been stored successfully! GaussianMixtureAlex_32_0.001_25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.01it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.08s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.07s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.08s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.07s/it]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.00it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.00s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.03s/it]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n",
            "100%|██████████| 6/6 [00:05<00:00,  1.00it/s]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.02s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.02s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.05s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.01s/it]\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint of {} has been stored successfully! GaussianMixtureAlex_32_0.001_50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Accuracy: 1.0\n",
            "Final Validation Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = split_data(data_path, batch_size = 32)"
      ],
      "metadata": {
        "id": "mU4r7RS2SxQp"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, val_loader, grey_images_flag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSB6cMLWR2E",
        "outputId": "14d68b7b-94c1-4b20-ce2b-7aa85306f55f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4, 5, 1, 2, 7, 5, 7, 3, 4, 6, 5]),\n",
              " [0.1360771,\n",
              "  0.10304779,\n",
              "  0.15784532,\n",
              "  0.2073363,\n",
              "  0.002631545,\n",
              "  0.08222133,\n",
              "  0.0047422647,\n",
              "  0.05213332,\n",
              "  0.12586898,\n",
              "  0.17192313,\n",
              "  0.098596275],\n",
              " array([0.3688863 , 0.32101056, 0.39729753, 0.45534196, 0.05129859,\n",
              "        0.28674263, 0.06886411, 0.22832723, 0.35478017, 0.41463614,\n",
              "        0.31400043], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Real time face detection & ID identification "
      ],
      "metadata": {
        "id": "Tv8JBa2EngEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper method for creating video streaming"
      ],
      "metadata": {
        "id": "26W3XnZzFSvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply): \n",
        "  image_bytes = b64decode(js_reply.split(',')[1])        # decode base64 image\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8) # convert bytes to numpy array\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)                 # decode numpy array into OpenCV BGR image\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(ipt):\n",
        "  # ipt: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  # bytes: Base64 image byte string\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(ipt, 'RGBA')\n",
        "  iobuf = io.BytesIO() \n",
        "  bbox_PIL.save(iobuf, format='png')                                                          # format bbox into png for return\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8'))) # format return string\n",
        "\n",
        "  return bbox_bytes\n",
        "  \n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;   \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 720, 720);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = '<span style=\"color: red; font-weight: bold;\">' + 'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 720; //video width; 1280\n",
        "      captureCanvas.height = 720; //video height; 720\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "jTpvCmAHeuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Face detection model implementations"
      ],
      "metadata": {
        "id": "e4m_f4VpszCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-implementing face detection model, in case some unexpected changes were made in part A"
      ],
      "metadata": {
        "id": "Io0rL7MZJLDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu0FP7V35jQR",
        "outputId": "b2b9c25e-824d-422f-e7cd-600e8c3719f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))\n",
        "# currently using, for face detection\n",
        "mtcnn = MTCNN(keep_all=True, min_face_size=224, device=device)\n",
        "# A faster model\n",
        "mtcnn_fast = FastMTCNN(keep_all=True, min_face_size=224, device=device,stride=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning on webcam, start real-time ID recognition"
      ],
      "metadata": {
        "id": "tpzpKIe6JYuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkSnkbkk4cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "aea89a80-6597-49d2-924f-ed690a29e72d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;   \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 720, 720);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = '<span style=\"color: red; font-weight: bold;\">' + 'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 720; //video width; 1280\n",
              "      captureCanvas.height = 720; //video height; 720\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torch.functional import Tensor\n",
        "video_stream()\n",
        "label_html = 'Capturing Video...'\n",
        "bbox = ''\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    \n",
        "    img = js_to_image(js_reply[\"img\"])                            #Calling helper method to convert JS response to OpenCV Image\n",
        "\n",
        "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    #frame_fast = torch.from_numpy(rgb_img) \n",
        "    #frame_fast = torch.stack([frame_fast], dim=0)                #Prepared input frame for mtcnn_fast face detection model\n",
        "    frame = Image.fromarray(rgb_img)                              #Prepared input frame for mtcnn face detection model\n",
        "\n",
        "    faces, _ = mtcnn.detect(frame)                                #Return coordinates of faces from mtcnn model\n",
        "    #faces_fast = mtcnn_fast(frame_fast)                          #Higher throughput model, return captured face images, not coordinates.\n",
        "    #plt.imshow(faces_fast[0].detach().numpy(), cmap = 'gray')    #Displaying images only for developing, commented out in actual practice.\n",
        "    #plt.show()\n",
        "\n",
        "    bbox_array = np.zeros([720,720,4], dtype=np.uint8)            #Creating a transparent overlay for drawing bounding box\n",
        "\n",
        "    if faces is not None: \n",
        "      for (x,y,w,h) in faces:\n",
        "        x_w_diff = int(w-x)\n",
        "        y_h_diff = int(h-y)\n",
        "        if x_w_diff > 223. and y_h_diff >223.:                    #Size filtering, detected faces need to be clear enough (224 x 224)\n",
        "          x_w_mid = int(x+(w-x)/2)                                #Finding the middle point along the x-axis\n",
        "          y_h_mid = int(y+(h-y)/2)                                #Finding the middle point along the y-axis\n",
        "          selected_x = x_w_mid - 140\n",
        "          selected_w = x_w_mid + 140\n",
        "          selected_y = y_h_mid - 260\n",
        "          selected_h = y_h_mid + 20                               \n",
        " \n",
        "          selected_img = img[selected_y:selected_h, selected_x:selected_w, :]  #Adjustment for cropping the correct face images.\n",
        "\n",
        "          faces = Image.fromarray(selected_img).resize((224,224))\n",
        "    \n",
        "          gray_img = cv2.cvtColor(np.array(faces), cv2.COLOR_RGB2GRAY)\n",
        "          #plt.imshow(gray_img, cmap = 'gray')                                 #Displaying face images during developing, commented out in actual practice\n",
        "          #plt.show()\n",
        "          \n",
        "          input_for_face_recognition = torch.from_numpy(gray_img) \n",
        "          input_for_face_recognition = torch.stack([input_for_face_recognition], dim=0) #Prepared input in tensor format, Ready for our ID detection model\n",
        "          \n",
        "\n",
        "          # Facial identification process will be placed here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "          box_height = (w,int(h-(20)))                                           #Adjusting the size of the bounding box to indicate detected faces.\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y), box_height, (0,255,0), 2) #Bounding box size and colour\n",
        "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255  \n",
        "      bbox_bytes = bbox_to_bytes(bbox_array)                                     # converting overlay of bbox into bytes  \n",
        "      bbox =bbox_bytes                                                           # update bbox for the next frame  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html Project_Face_detection.ipynb"
      ],
      "metadata": {
        "id": "3uKX1qnj0HVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d00412ee-9b98-44c6-c35a-9c06e5109192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'Project_Face_detection.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            `Exporter` class\n",
            "    Default: 'html'\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the \n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    overwrite base name use for output files.\n",
            "                can only be used when converting one notebook at a time.\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current \n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy \n",
            "            of reveal.js. \n",
            "            For speaker notes to work, this must be a relative path to a local \n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb\n",
            "\n",
            "            which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "\n",
            "            You can specify the export format with `--to`.\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "            can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of \n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-53e1b94cc7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jupyter nbconvert --to html Project_Face_detection.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'jupyter nbconvert --to html Project_Face_detection.ipynb' returned non-zero exit status 255."
          ]
        }
      ]
    }
  ]
}