{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Project_Face_detection_ensembled.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Real-time face detection & ID identification model for fast check-in."
   ],
   "metadata": {
    "id": "LyPFo1Fgm8IG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install necessary packages"
   ],
   "metadata": {
    "id": "IHS87y3cnSFr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyYCOhA9ei9M",
    "outputId": "1deea0f9-8a6a-4e9b-d31d-9d0a6dc8c3c6"
   },
   "outputs": [],
   "source": [
    "# %%shell\n",
    "# pip install facenet-pytorch\n",
    "# pip install mmcv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dependencies"
   ],
   "metadata": {
    "id": "S2mPaeV6naPG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Javascript, Image\n",
    "from IPython import display as dis\n",
    "# from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import numpy as np\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "from IPython.core.display import Video\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import mmcv, cv2\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import files\n",
    "# from google.colab import drive\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# from skimage.morphology import disk\n",
    "# from skimage.filters.rank import autolevel\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "SBvpY_rdeoLV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9c69560-411e-4201-aace-0342ef0fbb73"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
    "\n",
    "        faces = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            box_ind = int(i / self.stride)\n",
    "            if boxes[box_ind] is None:\n",
    "                continue\n",
    "            for box in boxes[box_ind]:\n",
    "                box = [int(b) for b in box]\n",
    "                faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
    "        \n",
    "        return faces"
   ],
   "metadata": {
    "id": "jyiKcDvYAV_l"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "# currently using, for face detection\n",
    "mtcnn = MTCNN(keep_all=True, min_face_size=224, device=device)\n",
    "# A faster model, for future performance improvment \n",
    "mtcnn_fast = FastMTCNN(keep_all=True, min_face_size=224, device=device,stride=4)"
   ],
   "metadata": {
    "id": "S5WwCg6O7Uwb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "88ad9550-4cd1-41d3-a5d2-e08c57c9b28b"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part A: Extracting training data from input videos."
   ],
   "metadata": {
    "id": "H6LqwC-MdH67"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# uploaded_video = files.upload() # For uploading training video to colab, click cancel if have no file to upload\n",
    "# user_name = \"ROY\"          #User name of the above uploaded video"
   ],
   "metadata": {
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 41
    },
    "id": "YGSswGlMdHFN",
    "outputId": "fe7411ba-480b-4847-cfd4-3f0102b9633d"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frames extraction: Extract each frame of user face, save to google drive"
   ],
   "metadata": {
    "id": "i-2_5mK_EPI-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "user_name = \"Zijian\"          #User name of the above uploaded video\n",
    "video = mmcv.VideoReader('E:/Codes/1517Project/Dataset/FacialDataset/OriginalVideo/' + user_name + '/' + user_name + '2.mp4') # Select training video here\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video] # Extracting frames from video\n",
    "frames_tracked = []\n",
    "path = 'E:/Codes/1517Project/Dataset/FacialDataset/ImageDataset/' + user_name  #Put in the output directory you want\n",
    "# os.mkdir(path)\n",
    "os.chdir(path)\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
    "    img = np.array(frame)    \n",
    "    faces, _ = mtcnn.detect(frame)                                           # Return coordinates of faces from mtcnn model\n",
    "\n",
    "    if i%2==0:  # change this to adjust sampling rate\n",
    "\n",
    "        if faces is not None:\n",
    "            for (x,y,w,h) in faces:\n",
    "                x_w_diff = int(w-x)\n",
    "                y_h_diff = int(h-y)\n",
    "                if x_w_diff > 223. and y_h_diff >223.:\n",
    "                    x_w_mid = int(x+(w-x)/2)\n",
    "                    y_h_mid = int(y+(h-y)/2)\n",
    "                    selected_x = x_w_mid - 155\n",
    "                    selected_w = x_w_mid + 155\n",
    "                    selected_y = y_h_mid - 260\n",
    "                    selected_h = y_h_mid + 50\n",
    "                    selected_img = img[selected_y:selected_h, selected_x:selected_w, :] #Adjustment for cropping the correct face images.\n",
    "\n",
    "                    faces = Image.fromarray(selected_img).resize((224,224))             #Resizing the image to desired input size.\n",
    "\n",
    "                    gray_img = cv2.cvtColor(np.array(faces), cv2.COLOR_RGB2GRAY)        #Converting to Gray Scale images for training\n",
    "                    cv2.imwrite(user_name + str(i) + '.png', gray_img)\n",
    "                    #plt.imshow(gray_img, cmap = 'gray')                                #Displaying images only for developing, commented out in actual practice.\n",
    "                    #plt.show()\n",
    "    else:\n",
    "      continue\n",
    "          \n",
    "\n",
    "print('\\nDone')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFUoWZkXfN80",
    "outputId": "387cf36f-90f1-4506-8d3e-fa8c2248ef5d"
   },
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 278\n",
      "Done\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ID identification MODEL and TRAINING method CAN be placed below."
   ],
   "metadata": {
    "id": "xF_d-_BTJiS_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use `ImageFolder` to load the categorical data. The label create by `ImageFolder` are integers begining from 0. We first see the correspondence of integer label to name label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bob': 0, 'DYY': 1, 'HZL': 2, 'JCL': 3, 'ML': 4, 'Roy': 5, 'Ruihua': 6, 'WPL': 7, 'ZC': 8, 'Zijian': 9, 'starlord': 10}\n"
     ]
    }
   ],
   "source": [
    "### See Imagefolder label\n",
    "dataset_path = 'E:/Codes/1517Project/Dataset/FacialDataset/ImageDataset/'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),  # (224, 224)\n",
    "                                    transforms.RandomHorizontalFlip(),  # flip images\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),])\n",
    "print(datasets.ImageFolder(root=dataset_path, transform=transform).class_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split_data(dataset_path, batch_size=64):\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                    transforms.RandomHorizontalFlip(),  # flip images\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),])\n",
    "\n",
    "    training_dataset = datasets.ImageFolder(root = dataset_path, transform=transform)\n",
    "\n",
    "    # print(training_dataset.class_to_idx)\n",
    "\n",
    "    indices = list(range(len(training_dataset)))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_train_val = int(0.8 * len(indices))\n",
    "\n",
    "    train_indices, val_indices = indices[:split_train_val], indices[split_train_val:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader, val_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, sampler=train_sampler), \\\n",
    "                  torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "                    \n",
    "    return train_loader, val_loader"
   ],
   "metadata": {
    "id": "yQzI9XK5JsIw"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AlexNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(AlexNetModel, self).__init__()\n",
    "        self.name = 'AlexNetModel'\n",
    "        self.conv1 = nn.Conv2d(256, 30, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(120, 64)\n",
    "        self.fc2 = nn.Linear(64, outputs*2)\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        mean, variance = torch.split(x, self.outputs, dim=1)\n",
    "        variance = F.softplus(variance) + 1e-6\n",
    "        return mean, variance\n"
   ],
   "metadata": {
    "id": "F7E6MnQJVY7E"
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GaussianMixtureAlex(nn.Module):\n",
    "    \"\"\" Gaussian mixture MLP which outputs are mean and variance.\n",
    "\n",
    "    Attributes:\n",
    "        models (int): number of models\n",
    "        inputs (int): number of inputs\n",
    "        outputs (int): number of outputs\n",
    "        hidden_layers (list of ints): hidden layer sizes\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_models=5, outputs=1):\n",
    "        super(GaussianMixtureAlex, self).__init__()\n",
    "        self.name = 'GaussianMixtureAlex'\n",
    "        self.num_models = num_models\n",
    "        self.outputs = outputs\n",
    "        for i in range(self.num_models):\n",
    "            model = AlexNetModel(outputs=self.outputs)\n",
    "            setattr(self, 'model_'+str(i), model)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # connect layers\n",
    "        means = []\n",
    "        variances = []\n",
    "        for i in range(self.num_models):\n",
    "            model = getattr(self, 'model_' + str(i))\n",
    "            mean, var = model(x)\n",
    "            means.append(mean)\n",
    "            variances.append(var)\n",
    "        means = torch.stack(means)\n",
    "        mean = means.mean(dim=0)\n",
    "        variances = torch.stack(variances)\n",
    "        variance = (variances + means.pow(2)).mean(dim=0) - mean.pow(2)\n",
    "        return mean, variance "
   ],
   "metadata": {
    "id": "BbXE2d7CUJ9E"
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# function to save checkpoint\n",
    "\n",
    "def OneHotEncoder(ls, class_num = 13):\n",
    "  unique_ls = list(set(ls))\n",
    "  onehot_encoded = np.zeros((len(ls), class_num))\n",
    "  for i in range(len(ls)):\n",
    "    onehot_encoded[i][ls[i]] = 1\n",
    "  return onehot_encoded\n",
    "\n",
    "def save_checkpoint(model, batch_size, lr, epoch):\n",
    "  model_path = \"{}_{}_{}_{}\".format(model.name, batch_size, lr, epoch)\n",
    "  torch.save(model, model_path)\n",
    "  print('Checkpoint of {} has been stored successfully!',format(model_path))\n",
    "\n",
    "def NLLloss(y, mean, var):\n",
    "    \"\"\" Negative log-likelihood loss function. \"\"\"\n",
    "    return (torch.log(var) + ((y - mean).pow(2))/var).sum()\n",
    "\n",
    "def get_accuracy(model, data_loader, grey_images_flag = False):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "  for imgs, labels in data_loader:\n",
    "    #############################################\n",
    "    #To Enable GPU Usage\n",
    "\n",
    "    if grey_images_flag:\n",
    "\n",
    "      grey_images = torchvision.transforms.Grayscale()(imgs)\n",
    "\n",
    "      imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
    "\n",
    "    imgs = alexnet.features(imgs)\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #############################################\n",
    "    \n",
    "    mean, _ = model(imgs)\n",
    "    \n",
    "    #select index with maximum prediction score\n",
    "    pred = mean.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    total += imgs.shape[0]\n",
    "  return correct / total\n",
    "\n",
    "def predict(model, data_loader, grey_images_flag = False):\n",
    "\n",
    "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "  pred_ls, var_ls, std_ls = [], [], []\n",
    "\n",
    "  for imgs, _ in data_loader:\n",
    "    #############################################\n",
    "    #To Enable GPU Usage\n",
    "\n",
    "    if grey_images_flag:\n",
    "\n",
    "      grey_images = torchvision.transforms.Grayscale()(imgs)\n",
    "\n",
    "      imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
    "\n",
    "    imgs = alexnet.features(imgs)\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "    #############################################\n",
    "    \n",
    "    mean, var = model(imgs)\n",
    "    \n",
    "    #select index with maximum prediction score\n",
    "    pred = torch.argmax(mean, dim=1)\n",
    "\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "\n",
    "    var = var.cpu().detach().numpy()\n",
    "\n",
    "    variance = [var[i][val] for i, val in enumerate(pred)]\n",
    "\n",
    "    std = np.sqrt(variance)\n",
    "\n",
    "    pred_ls = [*pred_ls, *pred]\n",
    "\n",
    "    var_ls = [*var_ls, *variance]\n",
    "\n",
    "    std_ls = [*std_ls, *std]\n",
    "\n",
    "  return pred_ls, var_ls, std_ls\n",
    "\n",
    "\n",
    "def train(model, dataset_path, opt, batch_size = 64, learning_rate=0.01, epochs=30, grey_images_flag = False):\n",
    "\n",
    "  #############################################\n",
    "  # input: grey_images_flag: boolean if gray image conversion is needed\n",
    "  #############################################\n",
    "\n",
    "  train_loader, val_loader = split_data(dataset_path, batch_size = batch_size)\n",
    "\n",
    "  criterion = NLLloss\n",
    "\n",
    "  # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "  iters, losses, train_acc, val_acc = [], [], [], []\n",
    "  \n",
    "  alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "  n = 0 # the number of iterations\n",
    "  for epoch in range(epochs):\n",
    "    for imgs, labels in iter(train_loader):\n",
    "\n",
    "      if grey_images_flag:\n",
    "\n",
    "        grey_images = torchvision.transforms.Grayscale()(imgs)\n",
    "\n",
    "        imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
    "\n",
    "      imgs = alexnet.features(imgs)\n",
    "\n",
    "      #############################################\n",
    "      #To Enable GPU Usage\n",
    "      if use_cuda and torch.cuda.is_available():\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "        one_hot_labels = torch.from_numpy(OneHotEncoder(labels))\n",
    "\n",
    "        labels = one_hot_labels.cuda()\n",
    "      #############################################\n",
    "  \n",
    "      mean, var = model(imgs) \n",
    "      # print(mean.shape)\n",
    "      # print(var.shape)\n",
    "      # print(labels.shape)\n",
    "\n",
    "      # output = model(imgs)\n",
    "\n",
    "      loss = NLLloss(labels, mean, var)\n",
    "\n",
    "      # loss = criterion(output, labels)\n",
    "\n",
    "      # loss = criterion(m(mean), labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    # save the current training information\n",
    "    iters.append(n)\n",
    "    losses.append(float(loss)/batch_size)\n",
    "    train_acc.append(get_accuracy(model, train_loader, grey_images_flag = grey_images_flag))\n",
    "    val_acc.append(get_accuracy(model, val_loader, grey_images_flag = grey_images_flag))\n",
    "    print('Epoch{}, Train acc: {} | Val acc: {} '\n",
    "              .format(epoch + 1, \"%.5f\" % train_acc[-1], \"%.5f\" % val_acc[-1]))\n",
    "    n += 1\n",
    "      \n",
    "    if ((epoch+1) % 25 == 0):\n",
    "      save_checkpoint(model, batch_size, learning_rate, epoch+1)\n",
    "\n",
    "  # plotting\n",
    "  plt.title(\"Training Curve\")\n",
    "  plt.plot(iters, losses, label=\"Train\")\n",
    "  plt.xlabel(\"Iterations\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.show()\n",
    "\n",
    "  plt.title(\"Training Curve\")\n",
    "  plt.plot(iters, train_acc, label=\"Train\")\n",
    "  plt.plot(iters, val_acc, label=\"Validation\")\n",
    "  plt.xlabel(\"Iterations\")\n",
    "  plt.ylabel(\"Training Accuracy\")\n",
    "  plt.legend(loc='best')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "  print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
   ],
   "metadata": {
    "id": "6HqngQ4wVjgq"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_num = 13\n",
    "\n",
    "model = GaussianMixtureAlex(outputs = class_num)\n",
    "\n",
    "data_path = 'E:/Codes/1517Project/Dataset/FacialDataset/ImageDataset/'\n",
    "\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "  print('CUDA is available!  Training on GPU ...')\n",
    "else:\n",
    "  print('CUDA is not available.  Training on CPU ...')\n",
    "  \n",
    "train(model, data_path, opt = optim.Adam, batch_size = 128, learning_rate=0.0001, epochs=50, grey_images_flag = True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "PRfl6tg9WMDe",
    "outputId": "f6ee8c92-8219-4841-b31d-e715dfd93218"
   },
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Epoch1, Train acc: 0.15300 | Val acc: 0.13514 \n",
      "Epoch2, Train acc: 0.19019 | Val acc: 0.22297 \n",
      "Epoch3, Train acc: 0.27134 | Val acc: 0.28041 \n",
      "Epoch4, Train acc: 0.43787 | Val acc: 0.41554 \n",
      "Epoch5, Train acc: 0.63990 | Val acc: 0.61149 \n",
      "Epoch6, Train acc: 0.82079 | Val acc: 0.74662 \n",
      "Epoch7, Train acc: 0.88081 | Val acc: 0.82432 \n",
      "Epoch8, Train acc: 0.92054 | Val acc: 0.87500 \n",
      "Epoch9, Train acc: 0.95858 | Val acc: 0.94257 \n",
      "Epoch10, Train acc: 0.95435 | Val acc: 0.94932 \n",
      "Epoch11, Train acc: 0.99239 | Val acc: 0.99324 \n",
      "Epoch12, Train acc: 0.98563 | Val acc: 0.98986 \n",
      "Epoch13, Train acc: 0.99155 | Val acc: 1.00000 \n",
      "Epoch14, Train acc: 0.99662 | Val acc: 0.99662 \n",
      "Epoch15, Train acc: 0.99493 | Val acc: 1.00000 \n",
      "Epoch16, Train acc: 0.99577 | Val acc: 1.00000 \n",
      "Epoch17, Train acc: 0.99746 | Val acc: 1.00000 \n",
      "Epoch18, Train acc: 0.99662 | Val acc: 1.00000 \n",
      "Epoch19, Train acc: 0.99662 | Val acc: 1.00000 \n",
      "Epoch20, Train acc: 0.99662 | Val acc: 1.00000 \n",
      "Epoch21, Train acc: 0.99577 | Val acc: 1.00000 \n",
      "Epoch22, Train acc: 0.99662 | Val acc: 1.00000 \n",
      "Epoch23, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch24, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch25, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Checkpoint of {} has been stored successfully! GaussianMixtureAlex_128_0.0001_25\n",
      "Epoch26, Train acc: 0.99831 | Val acc: 1.00000 \n",
      "Epoch27, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch28, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch29, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch30, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch31, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch32, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch33, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch34, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch35, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch36, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch37, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch38, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch39, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch40, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch41, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch42, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch43, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch44, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch45, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch46, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch47, Train acc: 0.99915 | Val acc: 1.00000 \n",
      "Epoch48, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch49, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Epoch50, Train acc: 1.00000 | Val acc: 1.00000 \n",
      "Checkpoint of {} has been stored successfully! GaussianMixtureAlex_128_0.0001_50\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 1.0\n",
      "Final Validation Accuracy: 1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader, val_loader = split_data(data_path, batch_size = 32)"
   ],
   "metadata": {
    "id": "mU4r7RS2SxQp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6e510cda-9e74-475b-ec9b-23e22377ae4c"
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pred, variance, std = predict(model, val_loader, grey_images_flag = True)"
   ],
   "metadata": {
    "id": "qnSB6cMLWR2E"
   },
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "([5,\n  3,\n  3,\n  12,\n  12,\n  9,\n  3,\n  6,\n  12,\n  3,\n  5,\n  9,\n  3,\n  12,\n  0,\n  10,\n  2,\n  12,\n  4,\n  11,\n  1,\n  11,\n  0,\n  12,\n  12,\n  5,\n  8,\n  5,\n  8,\n  12,\n  3,\n  3,\n  12,\n  5,\n  8,\n  8,\n  8,\n  1,\n  3,\n  10,\n  3,\n  9,\n  1,\n  11,\n  9,\n  6,\n  6,\n  6,\n  9,\n  4,\n  8,\n  3,\n  11,\n  0,\n  5,\n  1,\n  2,\n  9,\n  4,\n  0,\n  2,\n  8,\n  7,\n  0,\n  11,\n  3,\n  7,\n  0,\n  7,\n  12,\n  11,\n  9,\n  2,\n  2,\n  10,\n  6,\n  8,\n  1,\n  3,\n  6,\n  8,\n  2,\n  1,\n  8,\n  3,\n  8,\n  9,\n  11,\n  2,\n  10,\n  2,\n  7,\n  1,\n  12,\n  12,\n  11,\n  0,\n  11,\n  12,\n  5,\n  8,\n  10,\n  7,\n  1,\n  1,\n  7,\n  2,\n  3,\n  4,\n  7,\n  1,\n  6,\n  12,\n  6,\n  1,\n  4,\n  7,\n  12,\n  10,\n  8,\n  9,\n  7,\n  1,\n  8,\n  6,\n  5,\n  5,\n  4,\n  3,\n  10,\n  6,\n  4,\n  11,\n  5,\n  12,\n  8,\n  7,\n  9,\n  6,\n  4,\n  1,\n  2,\n  1,\n  11,\n  6,\n  1,\n  12,\n  3,\n  7,\n  0,\n  12,\n  4,\n  11,\n  10,\n  6,\n  8,\n  8,\n  11,\n  6,\n  7,\n  11,\n  6,\n  11,\n  2,\n  1,\n  1,\n  3,\n  2,\n  3,\n  11,\n  8,\n  11,\n  0,\n  3,\n  8,\n  3,\n  3,\n  2,\n  12,\n  3,\n  9,\n  11,\n  2,\n  4,\n  3,\n  10,\n  12,\n  3,\n  3,\n  9,\n  11,\n  6,\n  12,\n  8,\n  8,\n  1,\n  3,\n  3,\n  4,\n  10,\n  2,\n  4,\n  7,\n  1,\n  12,\n  0,\n  11,\n  12,\n  0,\n  7,\n  6,\n  8,\n  5,\n  11,\n  8,\n  9,\n  3,\n  9,\n  7,\n  3,\n  6,\n  8,\n  11,\n  0,\n  11,\n  2,\n  2,\n  3,\n  10,\n  11,\n  12,\n  9,\n  12,\n  9,\n  5,\n  12,\n  6,\n  2,\n  4,\n  10,\n  11,\n  4,\n  4,\n  2,\n  12,\n  7,\n  1,\n  6,\n  4,\n  10,\n  12,\n  8,\n  3,\n  10,\n  11,\n  3,\n  6,\n  12,\n  6,\n  10,\n  1,\n  9,\n  1,\n  7,\n  1,\n  1,\n  5,\n  8,\n  12,\n  7,\n  9,\n  4,\n  9,\n  6,\n  4,\n  1,\n  3,\n  1,\n  1,\n  11,\n  1,\n  12,\n  1,\n  8,\n  1,\n  0,\n  0,\n  7,\n  1,\n  9,\n  10,\n  12,\n  5,\n  4,\n  3,\n  11],\n [0.26428977,\n  0.15126419,\n  0.20651221,\n  0.13744614,\n  0.16312316,\n  0.16440599,\n  0.17653783,\n  0.17061263,\n  0.23547143,\n  0.176594,\n  0.25043893,\n  0.16564585,\n  0.19134077,\n  0.19897717,\n  0.18726578,\n  0.21485743,\n  0.13446389,\n  0.14156497,\n  0.23184022,\n  0.07435663,\n  0.22182533,\n  0.07489211,\n  0.119852796,\n  0.22319001,\n  0.27206698,\n  0.3183172,\n  0.16695203,\n  0.13607034,\n  0.19329372,\n  0.24877241,\n  0.19120845,\n  0.107614726,\n  0.22661068,\n  0.17076756,\n  0.19306552,\n  0.2010316,\n  0.13043098,\n  0.11717635,\n  0.1019732,\n  0.18081723,\n  0.11881042,\n  0.11218524,\n  0.20095861,\n  0.111481786,\n  0.14947496,\n  0.15452239,\n  0.21199718,\n  0.10673131,\n  0.08051535,\n  0.119205736,\n  0.14645067,\n  0.19767296,\n  0.06160507,\n  0.19598854,\n  0.19510484,\n  0.3184799,\n  0.1662125,\n  0.16330566,\n  0.12371383,\n  0.19999675,\n  0.23352423,\n  0.14551692,\n  0.15470038,\n  0.21412604,\n  0.10331259,\n  0.09944149,\n  0.17859283,\n  0.15775698,\n  0.13687667,\n  0.1636469,\n  0.10408377,\n  0.17213312,\n  0.2126521,\n  0.13075547,\n  0.13424006,\n  0.117846385,\n  0.19974783,\n  0.2772792,\n  0.1073225,\n  0.114705145,\n  0.12462528,\n  0.14336005,\n  0.14879271,\n  0.14308144,\n  0.15961388,\n  0.17631742,\n  0.06596481,\n  0.13943772,\n  0.15482545,\n  0.14402688,\n  0.2726553,\n  0.16654377,\n  0.15585573,\n  0.16718659,\n  0.12760594,\n  0.12441856,\n  0.20931783,\n  0.12970771,\n  0.1597605,\n  0.26117072,\n  0.17843108,\n  0.1266838,\n  0.09885603,\n  0.11803888,\n  0.3093989,\n  0.14791767,\n  0.22326334,\n  0.24501173,\n  0.20380987,\n  0.09887504,\n  0.119376846,\n  0.12663805,\n  0.23039687,\n  0.14263512,\n  0.27340794,\n  0.09595536,\n  0.13041666,\n  0.15041406,\n  0.25458753,\n  0.1689079,\n  0.1647011,\n  0.119959906,\n  0.14355758,\n  0.20548457,\n  0.11357008,\n  0.13776009,\n  0.23555616,\n  0.19822428,\n  0.2141642,\n  0.19675831,\n  0.13942394,\n  0.13691707,\n  0.1047201,\n  0.23532493,\n  0.16257711,\n  0.14751491,\n  0.089199685,\n  0.038102284,\n  0.11900692,\n  0.15587261,\n  0.18220308,\n  0.25866646,\n  0.12869634,\n  0.090487786,\n  0.16439193,\n  0.20433663,\n  0.13141978,\n  0.1686908,\n  0.13495639,\n  0.1495845,\n  0.17747776,\n  0.1957478,\n  0.1294376,\n  0.15280108,\n  0.17414974,\n  0.19054401,\n  0.22599441,\n  0.11975129,\n  0.13648227,\n  0.12523574,\n  0.115263,\n  0.22163194,\n  0.10725525,\n  0.24101858,\n  0.14524011,\n  0.262594,\n  0.12267869,\n  0.14912602,\n  0.15237992,\n  0.13424715,\n  0.19572183,\n  0.11337137,\n  0.16957867,\n  0.08097982,\n  0.16105023,\n  0.1542321,\n  0.18536419,\n  0.1678893,\n  0.20552836,\n  0.14983687,\n  0.12413746,\n  0.09953499,\n  0.12032146,\n  0.183464,\n  0.24234521,\n  0.18494688,\n  0.13153142,\n  0.21060158,\n  0.10750439,\n  0.09754073,\n  0.14710091,\n  0.213619,\n  0.24374472,\n  0.16363272,\n  0.24161619,\n  0.17372139,\n  0.14257936,\n  0.081555024,\n  0.16348061,\n  0.16374838,\n  0.21674697,\n  0.19983634,\n  0.12832367,\n  0.26312256,\n  0.15130645,\n  0.21646506,\n  0.15023354,\n  0.23908474,\n  0.22079776,\n  0.105550386,\n  0.17880505,\n  0.19391242,\n  0.07051057,\n  0.09076104,\n  0.16665718,\n  0.13420187,\n  0.2125761,\n  0.09014784,\n  0.16264147,\n  0.13825455,\n  0.14952295,\n  0.14550416,\n  0.08234892,\n  0.18770558,\n  0.1278413,\n  0.16950947,\n  0.2597431,\n  0.14045843,\n  0.19697979,\n  0.10428938,\n  0.20565073,\n  0.14852478,\n  0.21416856,\n  0.14555717,\n  0.17851473,\n  0.23941317,\n  0.13194336,\n  0.14602283,\n  0.1279651,\n  0.20252591,\n  0.14543612,\n  0.21855056,\n  0.19369914,\n  0.13322648,\n  0.24629398,\n  0.12589315,\n  0.09903395,\n  0.13377206,\n  0.17910708,\n  0.094611555,\n  0.22988312,\n  0.14197855,\n  0.076209754,\n  0.13611731,\n  0.11210935,\n  0.19823214,\n  0.12080523,\n  0.105736956,\n  0.1738944,\n  0.15443334,\n  0.11602108,\n  0.15089466,\n  0.096344456,\n  0.11157564,\n  0.09146452,\n  0.23502432,\n  0.18451914,\n  0.19532439,\n  0.24099874,\n  0.09540779,\n  0.1512817,\n  0.10634178,\n  0.1247454,\n  0.16616341,\n  0.23624434,\n  0.19353193,\n  0.106608115,\n  0.1855501,\n  0.19570656,\n  0.12853298,\n  0.168484,\n  0.17977512,\n  0.17220913,\n  0.17040814,\n  0.14621618,\n  0.19333291,\n  0.21598937,\n  0.16922593,\n  0.11778016,\n  0.14405826,\n  0.0970875,\n  0.12783056,\n  0.21353212,\n  0.20303889,\n  0.21896535,\n  0.11833666],\n [0.5140912,\n  0.38892698,\n  0.45443615,\n  0.37073728,\n  0.4038851,\n  0.40547007,\n  0.42016405,\n  0.41305283,\n  0.485254,\n  0.4202309,\n  0.50043875,\n  0.40699613,\n  0.43742517,\n  0.44606858,\n  0.43274218,\n  0.46352717,\n  0.3666932,\n  0.3762512,\n  0.48149788,\n  0.27268413,\n  0.47098336,\n  0.27366424,\n  0.34619763,\n  0.4724299,\n  0.5216004,\n  0.56419605,\n  0.40859765,\n  0.36887714,\n  0.43965182,\n  0.4987709,\n  0.4372739,\n  0.32804683,\n  0.47603643,\n  0.4132403,\n  0.4393922,\n  0.44836548,\n  0.3611523,\n  0.3423103,\n  0.31933242,\n  0.4252261,\n  0.3446889,\n  0.33494064,\n  0.44828406,\n  0.3338889,\n  0.38661993,\n  0.39309335,\n  0.46043152,\n  0.3266976,\n  0.28375226,\n  0.34526184,\n  0.38268873,\n  0.44460428,\n  0.24820368,\n  0.44270593,\n  0.44170675,\n  0.56434023,\n  0.40769166,\n  0.40411094,\n  0.35172978,\n  0.44720995,\n  0.48324347,\n  0.3814668,\n  0.3933197,\n  0.46273753,\n  0.32142276,\n  0.31534344,\n  0.42260244,\n  0.39718634,\n  0.36996847,\n  0.40453294,\n  0.32262015,\n  0.41488928,\n  0.46114218,\n  0.36160126,\n  0.36638784,\n  0.34328762,\n  0.44693157,\n  0.52657306,\n  0.32760113,\n  0.3386815,\n  0.35302305,\n  0.37862918,\n  0.38573658,\n  0.3782611,\n  0.39951706,\n  0.41990167,\n  0.25683615,\n  0.3734136,\n  0.39347866,\n  0.37950873,\n  0.52216405,\n  0.40809774,\n  0.39478567,\n  0.40888456,\n  0.35721976,\n  0.35273015,\n  0.45751265,\n  0.36014956,\n  0.39970052,\n  0.5110486,\n  0.42241102,\n  0.3559267,\n  0.3144138,\n  0.34356788,\n  0.5562364,\n  0.38460067,\n  0.4725075,\n  0.4949866,\n  0.45145306,\n  0.314444,\n  0.34550956,\n  0.3558624,\n  0.47999674,\n  0.37767065,\n  0.52288425,\n  0.30976662,\n  0.36113247,\n  0.38783252,\n  0.50456667,\n  0.41098407,\n  0.40583384,\n  0.34635228,\n  0.37888992,\n  0.45330405,\n  0.3370016,\n  0.37116045,\n  0.48534128,\n  0.44522384,\n  0.46277878,\n  0.4435745,\n  0.37339514,\n  0.37002307,\n  0.32360485,\n  0.485103,\n  0.40320852,\n  0.38407668,\n  0.29866317,\n  0.19519806,\n  0.3449738,\n  0.39480707,\n  0.42685252,\n  0.5085926,\n  0.3587427,\n  0.3008119,\n  0.40545276,\n  0.45203608,\n  0.36251867,\n  0.41071987,\n  0.3673641,\n  0.38676155,\n  0.4212811,\n  0.44243395,\n  0.35977438,\n  0.39089778,\n  0.4173125,\n  0.43651348,\n  0.4753887,\n  0.346051,\n  0.36943507,\n  0.3538866,\n  0.33950406,\n  0.47077802,\n  0.32749847,\n  0.49093643,\n  0.3811038,\n  0.51243925,\n  0.3502552,\n  0.3861684,\n  0.3903587,\n  0.36639753,\n  0.4424046,\n  0.33670667,\n  0.4117993,\n  0.28456953,\n  0.40131065,\n  0.39272395,\n  0.4305394,\n  0.40974295,\n  0.45335236,\n  0.38708767,\n  0.35233146,\n  0.31549165,\n  0.34687385,\n  0.428327,\n  0.4922857,\n  0.43005452,\n  0.3626726,\n  0.45891348,\n  0.32787862,\n  0.3123151,\n  0.38353735,\n  0.46218935,\n  0.4937051,\n  0.40451542,\n  0.4915447,\n  0.41679898,\n  0.37759683,\n  0.2855784,\n  0.40432736,\n  0.40465835,\n  0.4655609,\n  0.44703057,\n  0.35822293,\n  0.5129547,\n  0.3889813,\n  0.46525806,\n  0.3875997,\n  0.48896292,\n  0.46989122,\n  0.3248852,\n  0.42285347,\n  0.44035488,\n  0.26553828,\n  0.30126575,\n  0.40823668,\n  0.36633572,\n  0.46105978,\n  0.3002463,\n  0.4032883,\n  0.37182596,\n  0.38668197,\n  0.3814501,\n  0.286965,\n  0.43325,\n  0.357549,\n  0.41171527,\n  0.50965,\n  0.37477785,\n  0.44382405,\n  0.32293868,\n  0.4534873,\n  0.38538912,\n  0.4627835,\n  0.38151956,\n  0.42251003,\n  0.48929864,\n  0.3632401,\n  0.38212934,\n  0.35772207,\n  0.4500288,\n  0.3813609,\n  0.46749392,\n  0.44011265,\n  0.36500204,\n  0.49628013,\n  0.35481423,\n  0.3146966,\n  0.36574864,\n  0.42321044,\n  0.30758992,\n  0.47946128,\n  0.37680042,\n  0.27606115,\n  0.3689408,\n  0.33482733,\n  0.4452327,\n  0.34757048,\n  0.3251722,\n  0.4170065,\n  0.39298007,\n  0.34061867,\n  0.3884516,\n  0.31039402,\n  0.3340294,\n  0.30243102,\n  0.48479307,\n  0.4295569,\n  0.44195518,\n  0.49091622,\n  0.30888152,\n  0.38894948,\n  0.3261009,\n  0.35319313,\n  0.40763146,\n  0.4860497,\n  0.43992263,\n  0.32650897,\n  0.43075526,\n  0.44238734,\n  0.35851496,\n  0.41046804,\n  0.42399895,\n  0.4149809,\n  0.4128052,\n  0.38238224,\n  0.43969637,\n  0.46474656,\n  0.41137078,\n  0.34319115,\n  0.37955007,\n  0.31158867,\n  0.357534,\n  0.46209535,\n  0.45059836,\n  0.46793735,\n  0.34400097])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B: Real time face detection & ID identification "
   ],
   "metadata": {
    "id": "Tv8JBa2EngEs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper method for creating video streaming"
   ],
   "metadata": {
    "id": "26W3XnZzFSvJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply): \n",
    "  image_bytes = b64decode(js_reply.split(',')[1])        # decode base64 image\n",
    "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8) # convert bytes to numpy array\n",
    "  img = cv2.imdecode(jpg_as_np, flags=1)                 # decode numpy array into OpenCV BGR image\n",
    "\n",
    "  return img\n",
    "\n",
    "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "def bbox_to_bytes(ipt):\n",
    "  # ipt: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "  # bytes: Base64 image byte string\n",
    "  # convert array into PIL image\n",
    "  bbox_PIL = PIL.Image.fromarray(ipt, 'RGBA')\n",
    "  iobuf = io.BytesIO() \n",
    "  bbox_PIL.save(iobuf, format='png')                                                          # format bbox into png for return\n",
    "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8'))) # format return string\n",
    "\n",
    "  return bbox_bytes\n",
    "  \n",
    "# JavaScript to properly create our live video stream using our webcam as input\n",
    "def video_stream():\n",
    "  js = Javascript('''\n",
    "    var video;\n",
    "    var div = null;\n",
    "    var stream;\n",
    "    var captureCanvas;\n",
    "    var imgElement;\n",
    "    var labelElement;   \n",
    "    var pendingResolve = null;\n",
    "    var shutdown = false;\n",
    "    \n",
    "    function removeDom() {\n",
    "       stream.getVideoTracks()[0].stop();\n",
    "       video.remove();\n",
    "       div.remove();\n",
    "       video = null;\n",
    "       div = null;\n",
    "       stream = null;\n",
    "       imgElement = null;\n",
    "       captureCanvas = null;\n",
    "       labelElement = null;\n",
    "    }\n",
    "    \n",
    "    function onAnimationFrame() {\n",
    "      if (!shutdown) {\n",
    "        window.requestAnimationFrame(onAnimationFrame);\n",
    "      }\n",
    "      if (pendingResolve) {\n",
    "        var result = \"\";\n",
    "        if (!shutdown) {\n",
    "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 720, 720);\n",
    "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "        }\n",
    "        var lp = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        lp(result);\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    async function createDom() {\n",
    "      if (div !== null) {\n",
    "        return stream;\n",
    "      }\n",
    "\n",
    "      div = document.createElement('div');\n",
    "      div.style.border = '2px solid black';\n",
    "      div.style.padding = '3px';\n",
    "      div.style.width = '100%';\n",
    "      div.style.maxWidth = '600px';\n",
    "      document.body.appendChild(div);\n",
    "      \n",
    "      const modelOut = document.createElement('div');\n",
    "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
    "      labelElement = document.createElement('span');\n",
    "      labelElement.innerText = 'No data';\n",
    "      labelElement.style.fontWeight = 'bold';\n",
    "      modelOut.appendChild(labelElement);\n",
    "      div.appendChild(modelOut);\n",
    "           \n",
    "      video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      video.width = div.clientWidth - 6;\n",
    "      video.setAttribute('playsinline', '');\n",
    "      video.onclick = () => { shutdown = true; };\n",
    "      stream = await navigator.mediaDevices.getUserMedia(\n",
    "          {video: { facingMode: \"environment\"}});\n",
    "      div.appendChild(video);\n",
    "\n",
    "      imgElement = document.createElement('img');\n",
    "      imgElement.style.position = 'absolute';\n",
    "      imgElement.style.zIndex = 1;\n",
    "      imgElement.onclick = () => { shutdown = true; };\n",
    "      div.appendChild(imgElement);\n",
    "      \n",
    "      const instruction = document.createElement('div');\n",
    "      instruction.innerHTML = '<span style=\"color: red; font-weight: bold;\">' + 'When finished, click here or on the video to stop this demo</span>';\n",
    "      div.appendChild(instruction);\n",
    "      instruction.onclick = () => { shutdown = true; };\n",
    "      \n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      captureCanvas = document.createElement('canvas');\n",
    "      captureCanvas.width = 720; //video width; 1280\n",
    "      captureCanvas.height = 720; //video height; 720\n",
    "      window.requestAnimationFrame(onAnimationFrame);\n",
    "      \n",
    "      return stream;\n",
    "    }\n",
    "    async function stream_frame(label, imgData) {\n",
    "      if (shutdown) {\n",
    "        removeDom();\n",
    "        shutdown = false;\n",
    "        return '';\n",
    "      }\n",
    "\n",
    "      var preCreate = Date.now();\n",
    "      stream = await createDom();\n",
    "      \n",
    "      var preShow = Date.now();\n",
    "      if (label != \"\") {\n",
    "        labelElement.innerHTML = label;\n",
    "      }\n",
    "            \n",
    "      if (imgData != \"\") {\n",
    "        var videoRect = video.getClientRects()[0];\n",
    "        imgElement.style.top = videoRect.top + \"px\";\n",
    "        imgElement.style.left = videoRect.left + \"px\";\n",
    "        imgElement.style.width = videoRect.width + \"px\";\n",
    "        imgElement.style.height = videoRect.height + \"px\";\n",
    "        imgElement.src = imgData;\n",
    "      }\n",
    "      \n",
    "      var preCapture = Date.now();\n",
    "      var result = await new Promise(function(resolve, reject) {\n",
    "        pendingResolve = resolve;\n",
    "      });\n",
    "      shutdown = false;\n",
    "      \n",
    "      return {'create': preShow - preCreate, \n",
    "              'show': preCapture - preShow, \n",
    "              'capture': Date.now() - preCapture,\n",
    "              'img': result};\n",
    "    }\n",
    "    ''')\n",
    "\n",
    "  display(js)\n",
    "  \n",
    "def video_frame(label, bbox):\n",
    "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
    "  return data"
   ],
   "metadata": {
    "id": "jTpvCmAHeuCJ"
   },
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face detection model implementations"
   ],
   "metadata": {
    "id": "e4m_f4VpszCH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Re-implementing face detection model, in case some unexpected changes were made in part A"
   ],
   "metadata": {
    "id": "Io0rL7MZJLDS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pu0FP7V35jQR",
    "outputId": "573b44d4-ed3c-4382-a482-0e17fd61e35c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "# currently using, for face detection\n",
    "mtcnn = MTCNN(keep_all=True, min_face_size=224, device=device)\n",
    "# A faster model\n",
    "mtcnn_fast = FastMTCNN(keep_all=True, min_face_size=224, device=device,stride=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Turning on webcam, start real-time ID recognition"
   ],
   "metadata": {
    "id": "tpzpKIe6JYuj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"/content/GaussianMixtureAlex_32_0.001_25\")"
   ],
   "metadata": {
    "id": "qJn3xQ088330"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1nkSnkbkk4cC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "outputId": "b075fbc1-da59-436e-a271-4d16b4dcaefa"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    var video;\n",
       "    var div = null;\n",
       "    var stream;\n",
       "    var captureCanvas;\n",
       "    var imgElement;\n",
       "    var labelElement;   \n",
       "    var pendingResolve = null;\n",
       "    var shutdown = false;\n",
       "    \n",
       "    function removeDom() {\n",
       "       stream.getVideoTracks()[0].stop();\n",
       "       video.remove();\n",
       "       div.remove();\n",
       "       video = null;\n",
       "       div = null;\n",
       "       stream = null;\n",
       "       imgElement = null;\n",
       "       captureCanvas = null;\n",
       "       labelElement = null;\n",
       "    }\n",
       "    \n",
       "    function onAnimationFrame() {\n",
       "      if (!shutdown) {\n",
       "        window.requestAnimationFrame(onAnimationFrame);\n",
       "      }\n",
       "      if (pendingResolve) {\n",
       "        var result = \"\";\n",
       "        if (!shutdown) {\n",
       "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 720, 720);\n",
       "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
       "        }\n",
       "        var lp = pendingResolve;\n",
       "        pendingResolve = null;\n",
       "        lp(result);\n",
       "      }\n",
       "    }\n",
       "    \n",
       "    async function createDom() {\n",
       "      if (div !== null) {\n",
       "        return stream;\n",
       "      }\n",
       "\n",
       "      div = document.createElement('div');\n",
       "      div.style.border = '2px solid black';\n",
       "      div.style.padding = '3px';\n",
       "      div.style.width = '100%';\n",
       "      div.style.maxWidth = '600px';\n",
       "      document.body.appendChild(div);\n",
       "      \n",
       "      const modelOut = document.createElement('div');\n",
       "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
       "      labelElement = document.createElement('span');\n",
       "      labelElement.innerText = 'No data';\n",
       "      labelElement.style.fontWeight = 'bold';\n",
       "      modelOut.appendChild(labelElement);\n",
       "      div.appendChild(modelOut);\n",
       "           \n",
       "      video = document.createElement('video');\n",
       "      video.style.display = 'block';\n",
       "      video.width = div.clientWidth - 6;\n",
       "      video.setAttribute('playsinline', '');\n",
       "      video.onclick = () => { shutdown = true; };\n",
       "      stream = await navigator.mediaDevices.getUserMedia(\n",
       "          {video: { facingMode: \"environment\"}});\n",
       "      div.appendChild(video);\n",
       "\n",
       "      imgElement = document.createElement('img');\n",
       "      imgElement.style.position = 'absolute';\n",
       "      imgElement.style.zIndex = 1;\n",
       "      imgElement.onclick = () => { shutdown = true; };\n",
       "      div.appendChild(imgElement);\n",
       "      \n",
       "      const instruction = document.createElement('div');\n",
       "      instruction.innerHTML = '<span style=\"color: red; font-weight: bold;\">' + 'When finished, click here or on the video to stop this demo</span>';\n",
       "      div.appendChild(instruction);\n",
       "      instruction.onclick = () => { shutdown = true; };\n",
       "      \n",
       "      video.srcObject = stream;\n",
       "      await video.play();\n",
       "\n",
       "      captureCanvas = document.createElement('canvas');\n",
       "      captureCanvas.width = 720; //video width; 1280\n",
       "      captureCanvas.height = 720; //video height; 720\n",
       "      window.requestAnimationFrame(onAnimationFrame);\n",
       "      \n",
       "      return stream;\n",
       "    }\n",
       "    async function stream_frame(label, imgData) {\n",
       "      if (shutdown) {\n",
       "        removeDom();\n",
       "        shutdown = false;\n",
       "        return '';\n",
       "      }\n",
       "\n",
       "      var preCreate = Date.now();\n",
       "      stream = await createDom();\n",
       "      \n",
       "      var preShow = Date.now();\n",
       "      if (label != \"\") {\n",
       "        labelElement.innerHTML = label;\n",
       "      }\n",
       "            \n",
       "      if (imgData != \"\") {\n",
       "        var videoRect = video.getClientRects()[0];\n",
       "        imgElement.style.top = videoRect.top + \"px\";\n",
       "        imgElement.style.left = videoRect.left + \"px\";\n",
       "        imgElement.style.width = videoRect.width + \"px\";\n",
       "        imgElement.style.height = videoRect.height + \"px\";\n",
       "        imgElement.src = imgData;\n",
       "      }\n",
       "      \n",
       "      var preCapture = Date.now();\n",
       "      var result = await new Promise(function(resolve, reject) {\n",
       "        pendingResolve = resolve;\n",
       "      });\n",
       "      shutdown = false;\n",
       "      \n",
       "      return {'create': preShow - preCreate, \n",
       "              'show': preCapture - preShow, \n",
       "              'capture': Date.now() - preCapture,\n",
       "              'img': result};\n",
       "    }\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-43-422f89cb9768>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     59\u001B[0m           \u001B[0mfaces\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfaces\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m           \u001B[0malexnet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchvision\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malexnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m           \u001B[0mgrey_images\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchvision\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGrayscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfaces\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/alexnet.py\u001B[0m in \u001B[0;36malexnet\u001B[0;34m(pretrained, progress, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0mprogress\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIf\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisplays\u001B[0m \u001B[0ma\u001B[0m \u001B[0mprogress\u001B[0m \u001B[0mbar\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mdownload\u001B[0m \u001B[0mto\u001B[0m \u001B[0mstderr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m     \"\"\"\n\u001B[0;32m---> 62\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAlexNet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mpretrained\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/alexnet.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, num_classes)\u001B[0m\n\u001B[1;32m     35\u001B[0m         self.classifier = nn.Sequential(\n\u001B[1;32m     36\u001B[0m             \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m             \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m256\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m6\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4096\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m             \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReLU\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minplace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m             \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[1;32m     88\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mregister_parameter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bias'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreset_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mreset_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m         \u001B[0minit\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkaiming_uniform_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m             \u001B[0mfan_in\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minit\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calculate_fan_in_and_fan_out\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001B[0m in \u001B[0;36mkaiming_uniform_\u001B[0;34m(tensor, a, mode, nonlinearity)\u001B[0m\n\u001B[1;32m    393\u001B[0m     \u001B[0mbound\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3.0\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mstd\u001B[0m  \u001B[0;31m# Calculate uniform bounds from standard deviation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 395\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muniform_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mbound\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbound\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.functional import Tensor\n",
    "video_stream()\n",
    "label_html = 'Capturing Video...'\n",
    "bbox = ''\n",
    "while True:\n",
    "    js_reply = video_frame(label_html, bbox)\n",
    "    if not js_reply:\n",
    "        break\n",
    "    \n",
    "    img = js_to_image(js_reply[\"img\"])                            #Calling helper method to convert JS response to OpenCV Image\n",
    "\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #frame_fast = torch.from_numpy(rgb_img) \n",
    "    #frame_fast = torch.stack([frame_fast], dim=0)                #Prepared input frame for mtcnn_fast face detection model\n",
    "    frame = Image.fromarray(rgb_img)                              #Prepared input frame for mtcnn face detection model\n",
    "\n",
    "    faces, _ = mtcnn.detect(frame)                                #Return coordinates of faces from mtcnn model\n",
    "    #faces_fast = mtcnn_fast(frame_fast)                          #Higher throughput model, return captured face images, not coordinates.\n",
    "    #plt.imshow(faces_fast[0].detach().numpy(), cmap = 'gray')    #Displaying images only for developing, commented out in actual practice.\n",
    "    #plt.show()\n",
    "\n",
    "    bbox_array = np.zeros([720,720,4], dtype=np.uint8)            #Creating a transparent overlay for drawing bounding box\n",
    "\n",
    "    if faces is not None: \n",
    "      for (x,y,w,h) in faces:\n",
    "        x_w_diff = int(w-x)\n",
    "        y_h_diff = int(h-y)\n",
    "        if x_w_diff > 223. and y_h_diff >223.:                    #Size filtering, detected faces need to be clear enough (224 x 224)\n",
    "          x_w_mid = int(x+(w-x)/2)                                #Finding the middle point along the x-axis\n",
    "          y_h_mid = int(y+(h-y)/2)                                #Finding the middle point along the y-axis\n",
    "          selected_x = x_w_mid - 140\n",
    "          selected_w = x_w_mid + 140\n",
    "          selected_y = y_h_mid - 260\n",
    "          selected_h = y_h_mid + 20                               \n",
    " \n",
    "          selected_img = img[selected_y:selected_h, selected_x:selected_w, :]  #Adjustment for cropping the correct face images.\n",
    "\n",
    "          try:\n",
    "            faces = Image.fromarray(selected_img).resize((224,224))\n",
    "          except:\n",
    "            label_html = \"Move face to the center\"\n",
    "            continue\n",
    "    \n",
    "          gray_img = cv2.cvtColor(np.array(faces), cv2.COLOR_RGB2GRAY)\n",
    "          #plt.imshow(gray_img, cmap = 'gray')                                 #Displaying face images during developing, commented out in actual practice\n",
    "          #plt.show()\n",
    "          \n",
    "          input_for_face_recognition = torch.from_numpy(gray_img) \n",
    "          input_for_face_recognition = torch.stack([input_for_face_recognition], dim=0) #Prepared input in tensor format, Ready for our ID detection model\n",
    "          \n",
    "\n",
    "          # Facial identification process will be placed here\n",
    "\n",
    "          transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),  # flip images\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),])\n",
    "\n",
    "          faces = transform(faces)\n",
    "\n",
    "          alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "          grey_images = torchvision.transforms.Grayscale()(faces)\n",
    "\n",
    "          imgs = torch.tensor(np.tile(grey_images, [1,3,1,1]))\n",
    "          \n",
    "          imgs = alexnet.features(imgs)\n",
    "          if use_cuda and torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()\n",
    "          #############################################\n",
    "\n",
    "          mean, var = model(imgs)\n",
    "\n",
    "          confidence = nn.Softmax(dim=1)(mean)\n",
    "\n",
    "          confidence = torch.max(confidence)\n",
    "\n",
    "          #select index with maximum prediction score\n",
    "          pred = torch.argmax(mean, dim=1)\n",
    "          pred = pred.cpu().detach().numpy()\n",
    "          result = str(pred[0]) + \" Confidence {}  Uncertainty {}\".format(str(confidence.cpu().detach()) ,var[0][pred[0]])\n",
    "          # print(result)\n",
    "          label_html = result\n",
    "\n",
    "          box_height = (w,int(h-(20)))                                           #Adjusting the size of the bounding box to indicate detected faces.\n",
    "          bbox_array = cv2.rectangle(bbox_array,(x,y), box_height, (0,255,0), 2) #Bounding box size and colour\n",
    "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255  \n",
    "      bbox_bytes = bbox_to_bytes(bbox_array)                                     # converting overlay of bbox into bytes  \n",
    "      bbox =bbox_bytes                                                           # update bbox for the next frame  "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%shell\n",
    "jupyter nbconvert --to html Project_Face_detection.ipynb"
   ],
   "metadata": {
    "id": "3uKX1qnj0HVW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "Z6Rs0gufBXJY"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}